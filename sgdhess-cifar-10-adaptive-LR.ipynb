{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/tranhp98/SGDHess.git","metadata":{"execution":{"iopub.status.busy":"2022-12-24T13:35:36.969686Z","iopub.execute_input":"2022-12-24T13:35:36.970043Z","iopub.status.idle":"2022-12-24T13:35:39.214776Z","shell.execute_reply.started":"2022-12-24T13:35:36.970013Z","shell.execute_reply":"2022-12-24T13:35:39.213665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.4)\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim import Optimizer\n\nfrom typing import List, Optional\n\nimport torchvision\nfrom torchvision.models import resnet18, resnet34\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10\n\nfrom transformers import ResNetConfig, ResNetModel\n\nimport copy\nfrom tqdm.notebook import tqdm\n\nfrom IPython.display import clear_output\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-25T21:34:51.635857Z","iopub.execute_input":"2022-12-25T21:34:51.636205Z","iopub.status.idle":"2022-12-25T21:34:55.355425Z","shell.execute_reply.started":"2022-12-25T21:34:51.636176Z","shell.execute_reply":"2022-12-25T21:34:55.354281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SGDHess","metadata":{}},{"cell_type":"code","source":"class SGDHess:\n\n    def __init__(self, parameters, lr, momentum=1., clipping=None):\n        self.parameters = list(parameters)\n        self.lr = lr\n        self.momentum = momentum\n        self.clipping = clipping\n        \n        self.prev_params = None\n        self.direction = None\n        \n    def step(self, loss):\n        \n        if self.prev_params is None:\n            grad = torch.autograd.grad(outputs=loss, inputs=self.parameters)\n            self.prev_params = []\n            self.direction = []\n            with torch.no_grad():\n                for i, param in enumerate(self.parameters):\n                    self.prev_params.append(param.detach().clone())\n                    self.direction.append(grad[i].detach().clone())\n                    param -= self.lr * self.direction[i]\n            return        \n            \n        grad = torch.autograd.grad(outputs=loss, inputs=self.parameters, create_graph=True)\n        \n        \n        for i, param in enumerate(self.parameters):\n            self.prev_params[i].add_(param.detach(), alpha=-1)\n            \n        hessian = torch.autograd.grad(\n            outputs=grad, \n            inputs=self.parameters, \n            grad_outputs=self.prev_params\n        )\n        \n        for i, param in enumerate(self.parameters):\n            self.prev_params[i] = param.detach().clone()\n        \n        with torch.no_grad():\n            if self.clipping is not None:\n                sq_norm = torch.tensor(0.).to(self.parameters[0].device)\n                for i in range(len(self.direction)):\n                    self.direction[i].add_(self.direction[i], alpha=-self.momentum)\n                    self.direction[i].add_(hessian[i], alpha=-(1 - self.momentum))\n                    self.direction[i].add_(grad[i], alpha=self.momentum)\n                    sq_norm += (self.direction[i] ** 2).sum()\n\n                norm = torch.sqrt(sq_norm)\n                \n                if norm > self.clipping:\n                    for grad_i in self.direction:\n                        grad_i /= norm \n                        grad_i *= self.clipping\n            else:\n                for i in range(len(self.direction)):\n                    self.direction[i].add_(self.direction[i], alpha=-self.momentum)\n                    self.direction[i].add_(hessian[i], alpha=(1 - self.momentum))\n                    self.direction[i].add_(grad[i], alpha=self.momentum)\n            \n            for i, param in enumerate(self.parameters):\n                param.add_(self.direction[i], alpha=-self.lr)   \n                \n                \n                \n\nclass SGD:\n    \n    def __init__(self, parameters, lr, momentum=1., clipping=None):\n        self.parameters = list(parameters)\n        self.lr = lr\n        self.momentum = momentum\n        self.clipping = clipping\n        \n#         self.prev_params = None\n        self.direction = None\n        \n    def step(self, loss):\n        \n        if self.direction is None:\n            grad = torch.autograd.grad(outputs=loss, inputs=self.parameters)\n            self.prev_params = []\n            self.direction = []\n            with torch.no_grad():\n                for i, param in enumerate(self.parameters):\n#                     self.prev_params.append(param.detach().clone())\n                    self.direction.append(grad[i].detach().clone())\n                    param -= self.lr * self.direction[i]\n            return        \n            \n        grad = torch.autograd.grad(outputs=loss, inputs=self.parameters, create_graph=True)\n        \n#         for i, param in enumerate(self.parameters):\n#             self.prev_params[i].add_(param.detach(), alpha=-1)\n            \n#         hessian = torch.autograd.grad(\n#             outputs=grad, \n#             inputs=self.parameters, \n#             grad_outputs=self.prev_params\n#         )\n        \n#         for i, param in enumerate(self.parameters):\n#             self.prev_params[i] = param.detach().clone()\n        \n        with torch.no_grad():\n            if self.clipping is not None:\n                sq_norm = torch.tensor(0.).to(self.parameters[0].device)\n                for i in range(len(self.direction)):\n                    self.direction[i].add_(self.direction[i], alpha=-self.momentum)\n#                     self.direction[i].add_(hessian[i], alpha=- (1 - self.momentum))\n                    self.direction[i].add_(grad[i], alpha=self.momentum)\n                    sq_norm += (self.direction[i] ** 2).sum()\n\n                norm = torch.sqrt(sq_norm)\n                \n                if norm > self.clipping:\n                    for grad_i in self.direction:\n                        grad_i /= norm \n                        grad_i *= self.clipping\n            else:\n                for i in range(len(self.direction)):\n                    self.direction[i].add_(self.direction[i], alpha=-self.momentum)\n#                     self.direction[i].add_(hessian[i], alpha=- (1 - self.momentum))\n                    self.direction[i].add_(grad[i], alpha=self.momentum)\n            \n            for i, param in enumerate(self.parameters):\n                param.add_(self.direction[i], alpha=-self.lr)   \n                \n            ","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:34:55.358031Z","iopub.execute_input":"2022-12-25T21:34:55.358961Z","iopub.status.idle":"2022-12-25T21:34:55.385000Z","shell.execute_reply.started":"2022-12-25T21:34:55.358923Z","shell.execute_reply":"2022-12-25T21:34:55.384100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SGDHess_original(Optimizer):\n    \n    def __init__(self, params, lr=1e-2, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False, clip=False):\n        if lr and lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        self.clip = clip\n        self.iteration = -1\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n                        weight_decay=weight_decay, nesterov=nesterov)\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n        super(SGDHess_original, self).__init__(params, defaults)\n        for group in self.param_groups:\n            group.setdefault('nesterov', False)\n            for p in group['params']:\n                state = self.state[p]\n                state['displacement'] = torch.zeros_like(p)\n                state['max_grad'] = torch.zeros(1, device = p.device)\n                \n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n        self.iteration += 1\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n            vector = []\n            grads = []\n            param = []\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                vector.append(self.state[p]['displacement'])\n                grads.append(p.grad)\n                param.append(p)\n\n            hvp = torch.autograd.grad(outputs = grads, inputs = param, grad_outputs=vector)\n            with torch.no_grad():\n                i = 0\n                for p in group['params']:\n                    if p.grad is None:\n                        continue\n                    state = self.state[p]\n                    displacement, max_grad = state['displacement'], state['max_grad'] \n                    with torch.no_grad():\n                        d_p = p.grad\n                        if weight_decay != 0:\n                            d_p = d_p.add(p, alpha=weight_decay)\n                        if momentum != 0:\n                            if 'momentum_buffer' not in state:\n                                buf = state['momentum_buffer'] = torch.clone(d_p).detach()\n                            else:\n                                buf = state['momentum_buffer']\n                                buf.add_(hvp[i]).add_(displacement, alpha = weight_decay).mul_(momentum).add_(d_p, alpha=1 - dampening)\n                                if self.clip:\n                                    torch.nn.utils.clip_grad_norm_(buf, max_grad)\n                                    max_grad.copy_(torch.maximum((1-dampening)/(1-momentum)*torch.norm(d_p), max_grad))\n                            if nesterov:\n                                d_p = d_p.add(buf, alpha=momentum)\n                            else:\n                                d_p = buf\n                        displacement.copy_(d_p).mul_(-group['lr'])\n                        p.add_(displacement)\n                    i += 1\n                            \n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:34:55.386730Z","iopub.execute_input":"2022-12-25T21:34:55.387087Z","iopub.status.idle":"2022-12-25T21:34:55.405429Z","shell.execute_reply.started":"2022-12-25T21:34:55.387053Z","shell.execute_reply":"2022-12-25T21:34:55.404472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SGD_AdaptiveLR(Optimizer):\n\n    def __init__(self, params, lr=1e-2, momentum=1., dampening=0, c = 0.01, w = 10, k = 0.1,\n                 weight_decay=0, nesterov=False, clip=False, ):\n        if lr and lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        self.clip = clip\n        self.iteration = -1\n        self.c = c\n        self.w = w\n        self.momentum = momentum\n        defaults = dict(lr=lr, dampening=dampening, weight_decay=weight_decay, nesterov=nesterov,\n                        c = c, w = w, k = k)\n        \n        super(SGD_AdaptiveLR, self).__init__(params, defaults)\n        for group in self.param_groups:\n            group.setdefault('nesterov', False)\n            for p in group['params']:\n                state = self.state[p]\n                state['displacement'] = torch.zeros_like(p)\n                state['max_grad'] = torch.zeros(1, device = p.device)\n                state['G_1'] = torch.tensor(0., device = p.device)\n                state['G_2'] = torch.tensor(0., device = p.device)\n                state['G_cumsum'] = torch.tensor(0., device = p.device)\n                \n                \n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n        self.iteration += 1\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n            lr =  group['lr']\n            c = group['c']\n            w = group['w']\n            k = group['k']\n            vector = []\n            grads = []\n            param = []\n            \n            norm_sq = 0.\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                vector.append(self.state[p]['displacement'])\n                grads.append(p.grad)\n                param.append(p)\n                norm_sq += (p.grad ** 2).sum()\n\n            hvp = torch.autograd.grad(outputs = grads, inputs = param, grad_outputs=vector)\n            with torch.no_grad():\n                i = 0\n                for p in group['params']:\n                    if p.grad is None:\n                        continue\n                    state = self.state[p]\n                    displacement, max_grad = state['displacement'], state['max_grad']\n                    \n                    with torch.no_grad():\n                        d_p = p.grad\n                        if weight_decay != 0:\n                            d_p = d_p.add(p, alpha=weight_decay)\n\n                        if self.iteration == 0:\n                            buf = state['momentum_buffer'] = torch.clone(d_p).detach()\n                            state['G_cumsum'].add_(state['G_1'])\n                            state['G_1'], state['G_2'] = state['G_2'], (d_p ** 2).sum()\n                            state['lr'] = k/(torch.pow((w + state['G_cumsum']),1/3))\n                            state['momentum'] = self.momentum\n                        else:\n                            buf = state['momentum_buffer']\n                            state['G_cumsum'].add_(state['G_1'])\n                            state['G_1'], state['G_2'] = state['G_2'], (d_p ** 2).sum()\n                            state['lr'] = k / torch.pow(w +  state['G_cumsum'], 1 / 3)\n                            state['momentum'] = self.momentum\n                            buf.add_(hvp[i]).add_(displacement, alpha = weight_decay).mul_(state['momentum']).add_(d_p, alpha = 1-dampening)\n                            if nesterov:\n                                d_p = d_p.add(buf, alpha=state['momentum'])\n                            else:\n                                d_p = buf\n                                \n                        displacement.copy_(d_p).mul_(-state['lr'])\n                        p.add_(buf, alpha=-state['lr'])\n                        \n                        \n                    i += 1\n                            \n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:34:55.407810Z","iopub.execute_input":"2022-12-25T21:34:55.408156Z","iopub.status.idle":"2022-12-25T21:34:55.429291Z","shell.execute_reply.started":"2022-12-25T21:34:55.408121Z","shell.execute_reply":"2022-12-25T21:34:55.428330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data loading and preparation, Tiny ImageNet","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:34:55.649538Z","iopub.execute_input":"2022-12-25T21:34:55.649827Z","iopub.status.idle":"2022-12-25T21:34:55.654572Z","shell.execute_reply.started":"2022-12-25T21:34:55.649801Z","shell.execute_reply":"2022-12-25T21:34:55.653538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = '../output/'\ndataset = CIFAR10(root=DATA_PATH, train=True, transform=transform, download=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:34:56.060846Z","iopub.execute_input":"2022-12-25T21:34:56.061196Z","iopub.status.idle":"2022-12-25T21:35:02.249211Z","shell.execute_reply.started":"2022-12-25T21:34:56.061158Z","shell.execute_reply":"2022-12-25T21:35:02.248201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, val_data, test_data = torch.utils.data.random_split(dataset, [40000, 5000, 5000])","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:35:02.251038Z","iopub.execute_input":"2022-12-25T21:35:02.251386Z","iopub.status.idle":"2022-12-25T21:35:02.261179Z","shell.execute_reply.started":"2022-12-25T21:35:02.251353Z","shell.execute_reply":"2022-12-25T21:35:02.260291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 300\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\nval_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:35:02.262655Z","iopub.execute_input":"2022-12-25T21:35:02.263017Z","iopub.status.idle":"2022-12-25T21:35:02.270326Z","shell.execute_reply.started":"2022-12-25T21:35:02.262980Z","shell.execute_reply":"2022-12-25T21:35:02.269429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model creating","metadata":{}},{"cell_type":"code","source":"class CustomResNet20_c10(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        config = ResNetConfig()\n        resnet20 = ResNetModel(config)\n        self.embedder = resnet20\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Flatten(),\n            torch.nn.ReLU(),\n            torch.nn.Linear(2048, 200),\n        )\n        \n    def forward(self, X):\n        embeddings = self.embedder(X).pooler_output\n        return self.classifier(embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:35:02.272385Z","iopub.execute_input":"2022-12-25T21:35:02.272894Z","iopub.status.idle":"2022-12-25T21:35:02.280210Z","shell.execute_reply.started":"2022-12-25T21:35:02.272859Z","shell.execute_reply":"2022-12-25T21:35:02.279262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training, Tiny ImageNet","metadata":{}},{"cell_type":"code","source":"USE_GPU = True\n\ndtype = torch.float32\n\nif USE_GPU and torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\nprint('using device:', device)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:35:04.992422Z","iopub.execute_input":"2022-12-25T21:35:04.993312Z","iopub.status.idle":"2022-12-25T21:35:05.062980Z","shell.execute_reply.started":"2022-12-25T21:35:04.993265Z","shell.execute_reply":"2022-12-25T21:35:05.062044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CustomResNet20_c10()\nmodel.to(device)\nloss_function = torch.nn.CrossEntropyLoss()\ncustom_optimizer = SGDHess_original(model.parameters(), lr=1e-2, momentum=0.95, clip=True)\nnum_epochs = 200\ntrain_loss = []\ntrain_accuracy = []\ntrain_accuracy_5 = []\nval_accuracy = []\nval_aacuracy_5 = []\ntimes = []\nbest_val_accuracy = 0\nmean_train_loss = []","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:35:06.156221Z","iopub.execute_input":"2022-12-25T21:35:06.157095Z","iopub.status.idle":"2022-12-25T21:35:09.564790Z","shell.execute_reply.started":"2022-12-25T21:35:06.157059Z","shell.execute_reply":"2022-12-25T21:35:09.563820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_change_point = 0\n\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    \n    # Обучение\n    model.train(True)\n    \n    acc_train_5 = 0\n    acc_train = 0\n    \n    for X_batch, y_batch in tqdm(train_dataloader):\n        logits = model(X_batch.to(device))\n        loss = loss_function(logits, y_batch.to(device))\n        custom_optimizer.zero_grad()\n        \n        predictions = torch.argmax(logits, dim=1)\n        acc_train += ((predictions == y_batch.to(device)).sum().item() / X_batch.shape[0])\n        loss.backward(create_graph=True)\n        custom_optimizer.step()\n        train_loss.append(loss.item())\n        \n    times.append(time.time() - start_time)\n        \n    train_accuracy.append(\n        acc_train / len(train_dataloader)\n    )\n    \n    \n    # Валидация\n    model.train(False)\n\n    val_acc = 0\n    val_acc_5 = 0\n\n    for X_batch, y_batch in val_dataloader:\n        probabilities = model(X_batch.to(device))\n        predictions = torch.argmax(probabilities, dim=1)\n        val_acc += ((predictions == y_batch.to(device)).sum().item() / X_batch.shape[0])\n        \n        \n    val_accuracy.append(\n        val_acc / len(val_dataloader)\n    )\n    \n\n    epoch_val_accuracy = val_accuracy[-1] * 100\n    \n    if len(val_accuracy) > 3 and np.std(val_accuracy[-3:]) < 0.001 and epoch - lr_change_point >= 5:\n        lr_change_point = epoch\n        print('lr change')\n        for group in custom_optimizer.param_groups:\n            for p in group['params']:\n                group['lr'] /= 2.\n        \n\n    if (epoch_val_accuracy > best_val_accuracy):\n        torch.save(model.state_dict(), 'best_model.ml') \n        best_val_accuracy = epoch_val_accuracy\n\n    clear_output(True)\n\n    plt.figure(figsize=(20, 8))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(train_loss)\n    plt.xlabel('номер батча')\n    plt.ylabel('значение')\n    plt.title('Функция потерь')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(val_accuracy, label='val top 1')\n#     plt.plot(val_aacuracy_5, label='val top 5')\n    plt.plot(train_accuracy, label='train top 1')\n#     plt.plot(train_accuracy_5, label='train top 5')\n    plt.legend()\n    plt.xlabel('Номер эпохи')\n    plt.ylabel('Точность')\n    plt.show()\n        \n    print(\"Epoch {} of {} took {:.3f}s\".format(\n        epoch + 1, num_epochs, time.time() - start_time))\n    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n        np.mean(train_loss[-len(train_data) // batch_size :])))\n    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n            epoch_val_accuracy\n        ))\n    \n    mean_train_loss.append(np.mean(train_loss[-len(train_data) // batch_size :]))\n    \n    logs = {\n        'val top 1': val_accuracy,\n        'train top 1': train_accuracy,\n        'epoch_time': times\n    }\n    \n    torch.save(logs, 'logs_hess.pt')","metadata":{"execution":{"iopub.status.busy":"2022-12-25T20:44:18.789939Z","iopub.execute_input":"2022-12-25T20:44:18.790305Z","iopub.status.idle":"2022-12-25T20:58:38.007769Z","shell.execute_reply.started":"2022-12-25T20:44:18.790275Z","shell.execute_reply":"2022-12-25T20:58:37.994507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CustomResNet20_c10()\nmodel.to(device)\nloss_function = torch.nn.CrossEntropyLoss()\ncustom_optimizer = SGD(model.parameters(), 0.1, 0.8)\nnum_epochs = 200\ntrain_loss = []\ntrain_accuracy = []\ntrain_accuracy_5 = []\nval_accuracy = []\nval_aacuracy_5 = []\ntimes = []\nbest_val_accuracy = 0","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:04:48.554945Z","iopub.status.idle":"2022-12-22T20:04:48.555334Z","shell.execute_reply.started":"2022-12-22T20:04:48.555165Z","shell.execute_reply":"2022-12-22T20:04:48.555183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    \n    # Обучение\n    model.train(True)\n    \n    acc_train_5 = 0\n    acc_train = 0\n    \n    for X_batch, y_batch in tqdm(train_dataloader):\n        logits = model(X_batch.to(device))\n        loss = loss_function(logits, y_batch.to(device))\n        \n        predictions = torch.argmax(logits, dim=1)\n        topk = torch.topk(logits, k=5, dim=1).indices\n        acc_train += ((predictions == y_batch.to(device)).sum().item() / X_batch.shape[0])\n        for k in range(5):\n            acc_train_5 += (topk[:, k] == y_batch.to(device)).sum().item() / X_batch.shape[0]\n        custom_optimizer.step(loss)\n        train_loss.append(loss.item())\n        \n    times.append(time.time() - start_time)\n        \n    train_accuracy.append(\n        acc_train / len(train_dataloader)\n    )\n    \n    train_accuracy_5.append(\n        acc_train_5 / len(train_dataloader)\n    )\n    \n    \n    # Валидация\n    model.train(False)\n\n    val_acc = 0\n    val_acc_5 = 0\n\n    for X_batch, y_batch in val_dataloader:\n        probabilities = model(X_batch.to(device))\n        predictions = torch.argmax(probabilities, dim=1)\n        val_acc += ((predictions == y_batch.to(device)).sum().item() / X_batch.shape[0])\n        \n        topk = torch.topk(probabilities, k=5, dim=1).indices\n        for k in range(5):\n            val_acc_5 += (topk[:, k] == y_batch.to(device)).sum().item() / X_batch.shape[0]\n        \n    val_accuracy.append(\n        val_acc / len(val_dataloader)\n    )\n    \n    val_aacuracy_5.append(\n        val_acc_5 / len(val_dataloader)\n    )\n\n    epoch_val_accuracy = val_accuracy[-1] * 100\n\n    if (epoch_val_accuracy > best_val_accuracy):\n        torch.save(model.state_dict(), 'best_model.ml') \n        best_val_accuracy = epoch_val_accuracy\n        \n    if len(val_accuracy) > 3 and np.std(val_accuracy[-3:]) < 0.001 and epoch - lr_change_point >= 5:\n        lr_change_point = epoch\n        print('lr change')\n        for group in custom_optimizer.param_groups:\n            for p in group['params']:\n                group['lr'] /= 2.\n\n    clear_output(True)\n\n    plt.figure(figsize=(20, 8))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(train_loss)\n    plt.xlabel('номер батча')\n    plt.ylabel('значение')\n    plt.title('Функция потерь')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(val_accuracy, label='val top 1')\n    plt.plot(val_aacuracy_5, label='val top 5')\n    plt.plot(train_accuracy, label='train top 1')\n    plt.plot(train_accuracy_5, label='train top 5')\n    plt.legend()\n    plt.xlabel('Номер эпохи')\n    plt.ylabel('Точность')\n    plt.show()\n        \n    print(\"Epoch {} of {} took {:.3f}s\".format(\n        epoch + 1, num_epochs, time.time() - start_time))\n    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n        np.mean(train_loss[-len(train_data) // batch_size :])))\n    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n            epoch_val_accuracy\n        ))\n    \n    logs = {\n        'val top 1': val_accuracy,\n        'val top 2': val_aacuracy_5,\n        'train top 1': train_accuracy,\n        'train top 5': train_accuracy_5,\n        'epoch_time': times\n    }\n    \n    torch.save(logs, 'logs_sgd.pt')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:04:48.559546Z","iopub.status.idle":"2022-12-22T20:04:48.560064Z","shell.execute_reply.started":"2022-12-22T20:04:48.559789Z","shell.execute_reply":"2022-12-22T20:04:48.559812Z"},"trusted":true},"execution_count":null,"outputs":[]}]}